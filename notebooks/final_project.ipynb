{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeffVerastegui/AML-Final-Verastegui-Jefferson/blob/main/notebooks/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpo4SNkIK1tk"
      },
      "source": [
        "# üéì Capstone Project - Advanced Machine Learning\n",
        "## TEC-VIII Programa de Especializaci√≥n en Big Data Analytics aplicada a los Negocios\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Informaci√≥n del Proyecto\n",
        "\n",
        "| Campo | Informaci√≥n |\n",
        "|-------|-------------|\n",
        "| **Nombre del Estudiante** | [Jefferson Ver√°stegui] |\n",
        "| **T√≠tulo del Proyecto** | [Modelo de detecci√≥n y prevenci√≥n de fraudes en una billetera digital peruana] |\n",
        "| **Fecha de Entrega** | [19/02/2026] |\n",
        "| **Profesor** | [Mg. Ing. Carlo Mari√±o] |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbrpzlKRK1tn"
      },
      "source": [
        "## üìë √çndice\n",
        "\n",
        "1. [Resumen Ejecutivo](#1-resumen-ejecutivo)\n",
        "2. [Configuraci√≥n del Entorno](#2-configuraci√≥n-del-entorno)\n",
        "3. [Definici√≥n del Problema de Negocio](#3-definici√≥n-del-problema-de-negocio)\n",
        "4. [Carga y Exploraci√≥n de Datos](#4-carga-y-exploraci√≥n-de-datos)\n",
        "5. [Preprocesamiento de Datos](#5-preprocesamiento-de-datos)\n",
        "6. [Dise√±o y Arquitectura del Modelo](#6-dise√±o-y-arquitectura-del-modelo)\n",
        "7. [Entrenamiento del Modelo](#7-entrenamiento-del-modelo)\n",
        "8. [Evaluaci√≥n y M√©tricas](#8-evaluaci√≥n-y-m√©tricas)\n",
        "9. [Interpretaci√≥n de Resultados](#9-interpretaci√≥n-de-resultados)\n",
        "10. [Conclusiones y Recomendaciones de Negocio](#10-conclusiones-y-recomendaciones-de-negocio)\n",
        "11. [Referencias](#11-referencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqwolwpmK1to"
      },
      "source": [
        "---\n",
        "## 1. Resumen Ejecutivo\n",
        "\n",
        "**Instrucciones:** Proporcione un resumen conciso (m√°ximo 300 palabras) que incluya:\n",
        "- Problema de negocio abordado\n",
        "- Metodolog√≠a utilizada\n",
        "- Principales hallazgos\n",
        "- Impacto esperado en el negocio\n",
        "\n",
        "---\n",
        "\n",
        "Este proyecto se centra en la identificaci√≥n de transacciones fraudulentas dentro del √°mbito financiero, un reto relevante para las organizaciones debido a su impacto en las p√©rdidas econ√≥micas, la confianza de los clientes y los costos operativos. El prop√≥sito principal consiste en dise√±ar e implementar un pipeline escalable de Deep Learning en la nube capaz de detectar de forma temprana operaciones con alta probabilidad de fraude, utilizando un conjunto de datos real compuesto por cinco millones de registros de transacciones.\n",
        "\n",
        "La propuesta metodol√≥gica sigue un enfoque integral end-to-end que abarca desde la ingesta y exploraci√≥n inicial de los datos hasta el preprocesamiento de informaci√≥n estructurada, la normalizaci√≥n de variables num√©ricas y la partici√≥n del dataset en conjuntos de entrenamiento, validaci√≥n y prueba. Para la fase de modelado se desarroll√≥ una arquitectura Multi-Layer Perceptron (MLP), elegida por su buen desempe√±o con datos tabulares a gran escala y su eficiencia computacional. El entrenamiento se ejecut√≥ en un entorno cloud con GPU, lo que permiti√≥ acelerar el proceso y gestionar grandes vol√∫menes de informaci√≥n. La evaluaci√≥n del desempe√±o se realiz√≥ mediante m√©tricas clave para detecci√≥n de fraude, tales como AUC-ROC, precisi√≥n, recall y F1-score.\n",
        "\n",
        "Los resultados obtenidos muestran que el modelo posee una elevada capacidad para diferenciar entre transacciones leg√≠timas y fraudulentas, resaltando un recall alto en la clase minoritaria, aspecto fundamental para reducir fraudes no detectados. Adem√°s, el pipeline propuesto evidencia caracter√≠sticas de reproducibilidad, escalabilidad y viabilidad para entornos productivos.\n",
        "\n",
        "Desde la perspectiva del negocio, se espera que la soluci√≥n contribuya a disminuir p√©rdidas financieras, optimizar los procesos de monitoreo y fortalecer la gesti√≥n del riesgo, estableciendo una base s√≥lida para futuras mejoras e integraci√≥n con sistemas de prevenci√≥n de fraude en tiempo real.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv0YN9RmK1tp"
      },
      "source": [
        "## 2. Configuraci√≥n del Entorno\n",
        "\n",
        "### 2.1 Verificaci√≥n de GPU (Recomendado para Deep Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfbbrBotK1tp"
      },
      "outputs": [],
      "source": [
        "# Verificar si hay GPU disponible\n",
        "import torch\n",
        "\n",
        "# Verificar disponibilidad de GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU no disponible. Usando CPU.\")\n",
        "    print(\"   Recomendaci√≥n: En Colab, vaya a Runtime > Change runtime type > GPU\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"\\nDispositivo seleccionado: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es0Zw5rtK1tq"
      },
      "source": [
        "### 2.2 Instalaci√≥n de Librer√≠as Adicionales (si es necesario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S21HTZ6RK1tr"
      },
      "outputs": [],
      "source": [
        "# Descomente e instale las librer√≠as adicionales que necesite\n",
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning\n",
        "# !pip install optuna\n",
        "# !pip install shap\n",
        "# !pip install lime\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqvS8bggK1tr"
      },
      "source": [
        "### 2.3 Importaci√≥n de Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSHtW0YYK1ts"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# LIBRER√çAS FUNDAMENTALES\n",
        "# =====================================================\n",
        "\n",
        "# Manipulaci√≥n de datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep Learning - PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "# Deep Learning - TensorFlow/Keras (alternativa)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Preprocesamiento\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "\n",
        "# Utilidades\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWjRRcOcK1tt"
      },
      "source": [
        "### 2.4 Conexi√≥n con Google Drive (para cargar datos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL0RW9wjK1tt"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive para acceder a los datos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir la ruta base de su proyecto\n",
        "# Modifique esta ruta seg√∫n la ubicaci√≥n de sus datos\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/financial_fraud_detection_dataset.csv'\n",
        "\n",
        "print(f\"‚úÖ Google Drive montado\")\n",
        "print(f\"   Ruta base del proyecto: {BASE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhNIv7G7K1tu"
      },
      "source": [
        "---\n",
        "## 3. Definici√≥n del Problema de Negocio\n",
        "\n",
        "### 3.1 Contexto del Negocio\n",
        "\n",
        "**Instrucciones:** Describa el contexto empresarial, incluyendo:\n",
        "- Industria/Sector\n",
        "- Empresa o caso de estudio\n",
        "- Situaci√≥n actual\n",
        "\n",
        "---\n",
        "\n",
        "El presente proyecto se enmarca dentro del sector financiero, particularmente en organizaciones que gestionan altos vol√∫menes de transacciones electr√≥nicas, tales como entidades bancarias, fintechs y procesadores de pagos. En este contexto, las operaciones se generan de manera constante y a gran escala, incrementando la vulnerabilidad frente a actividades fraudulentas. El an√°lisis se sustenta en un dataset real anonimizado de transacciones financieras, dise√±ado para reflejar situaciones reales de detecci√≥n de fraude similares a las que enfrenta la industria.\n",
        "\n",
        "En la actualidad, numerosas organizaciones a√∫n utilizan enfoques tradicionales de detecci√≥n, basados en reglas predefinidas o procesos de revisi√≥n manual, los cuales presentan limitaciones ante el crecimiento exponencial de los datos y la sofisticaci√≥n de los esquemas fraudulentos. Estas estrategias suelen carecer de escalabilidad, implican elevados costos operativos y muestran poca capacidad de adaptaci√≥n frente a patrones de fraude din√°micos, lo que impulsa la adopci√≥n de soluciones sustentadas en anal√≠tica avanzada y t√©cnicas de Deep Learning.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Problema a Resolver\n",
        "\n",
        "**Instrucciones:** Defina claramente:\n",
        "- ¬øCu√°l es el problema espec√≠fico?\n",
        "- ¬øPor qu√© es importante resolverlo?\n",
        "- ¬øCu√°l es el impacto actual del problema?\n",
        "\n",
        "---\n",
        "\n",
        "La problem√°tica principal radica en la detecci√≥n precisa y oportuna de transacciones fraudulentas dentro de grandes vol√∫menes de operaciones financieras, donde predominan ampliamente las transacciones leg√≠timas. Este marcado desbalance entre clases representa un desaf√≠o significativo para los modelos de detecci√≥n, ya que reduce la efectividad en la identificaci√≥n del fraude y eleva el riesgo de p√©rdidas econ√≥micas, adem√°s de impactar negativamente en la experiencia del cliente.\n",
        "\n",
        "La ausencia de un modelo predictivo s√≥lido puede ocasionar tanto fraudes que pasan desapercibidos, incrementando el impacto financiero, como un aumento de falsos positivos que derivan en bloqueos innecesarios y mayor carga operativa para las √°reas de control. En este contexto, se vuelve esencial implementar una soluci√≥n automatizada, escalable y basada en anal√≠tica avanzada que permita fortalecer la capacidad de detecci√≥n, optimizar los procesos de monitoreo y mejorar la gesti√≥n del riesgo en entornos financieros con alta demanda transaccional.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Objetivos del Proyecto\n",
        "\n",
        "**Instrucciones:** Liste los objetivos SMART (Espec√≠ficos, Medibles, Alcanzables, Relevantes, Temporales)\n",
        "\n",
        "---\n",
        "\n",
        "**Objetivo General:**\n",
        "Dise√±ar e implementar un pipeline de Deep Learning escalable, capaz de identificar transacciones fraudulentas de manera automatizada y eficiente, mediante el procesamiento de grandes vol√∫menes de datos financieros y su ejecuci√≥n en un entorno cloud con c√≥mputo acelerado.\n",
        "\n",
        "**Objetivos Espec√≠ficos:**\n",
        "1. Realizar la preparaci√≥n y el preprocesamiento de un conjunto de datos financieros compuesto por m√°s de cinco millones de registros, garantizando la integridad, calidad y coherencia de la informaci√≥n.\n",
        "2. Dise√±ar, implementar y entrenar un modelo de Deep Learning basado en una arquitectura MLP orientado a la clasificaci√≥n de transacciones fraudulentas y leg√≠timas.\n",
        "3. Analizar y validar el desempe√±o del modelo mediante m√©tricas clave para problemas de fraude, tales como AUC-ROC, precisi√≥n, recall y F1-score, otorgando especial √©nfasis a la correcta identificaci√≥n de la clase minoritaria.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.4 Tipo de Problema de Machine Learning\n",
        "\n",
        "**Instrucciones:** Identifique el tipo de problema:\n",
        "- [X] ***Clasificaci√≥n binaria***\n",
        "- [ ] Clasificaci√≥n multiclase\n",
        "- [ ] Regresi√≥n\n",
        "- [ ] Clustering\n",
        "- [ ] Series temporales\n",
        "- [ ] Otro: _________\n",
        "\n",
        "**Justificaci√≥n:**\n",
        "El prop√≥sito del proyecto consiste en clasificar cada transacci√≥n dentro de una de dos categor√≠as: fraudulenta o leg√≠tima. Considerando que las variables de entrada corresponden a datos num√©ricos estructurados y que la variable objetivo es discreta de naturaleza binaria, el enfoque de clasificaci√≥n binaria resulta el m√°s apropiado. Esta formulaci√≥n permite optimizar indicadores clave para el negocio, como el recall en la clase fraudulenta, y se ajusta a las metodolog√≠as de Deep Learning empleadas en sistemas modernos de detecci√≥n de fraude a gran escala.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyesEZrQK1tv"
      },
      "source": [
        "---\n",
        "## 4. Carga y Exploraci√≥n de Datos\n",
        "\n",
        "### 4.1 Carga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HGW5f-nK1tv"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CARGA DE DATOS\n",
        "# =====================================================\n",
        "\n",
        "# Opci√≥n 1: Cargar desde Google Drive\n",
        "# df = pd.read_csv(BASE_PATH + 'datos.csv')\n",
        "\n",
        "# Opci√≥n 2: Cargar desde URL\n",
        "# df = pd.read_csv('https://url-de-sus-datos.com/datos.csv')\n",
        "\n",
        "# Opci√≥n 3: Cargar desde archivo local (subido a Colab)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# df = pd.read_csv('nombre_archivo.csv')\n",
        "\n",
        "# Opci√≥n 4: Dataset de ejemplo (para testing)\n",
        "# from sklearn.datasets import load_iris, load_boston, fetch_california_housing\n",
        "# data = load_iris()\n",
        "# df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# df['target'] = data.target\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Cargue su dataset\n",
        "# =====================================================\n",
        "\n",
        "# df = pd.read_csv('...')  # Descomente y complete\n",
        "df = pd.read_csv(BASE_PATH)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
        "print(f\"   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tld9-TzXK1tw"
      },
      "source": [
        "### 4.2 Descripci√≥n del Dataset\n",
        "\n",
        "**Instrucciones:** Describa su dataset:\n",
        "- Fuente de los datos\n",
        "- Per√≠odo de tiempo que cubren\n",
        "- Descripci√≥n de cada variable\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "**Fuente:**\n",
        "El dataset utilizado proviene de Kaggle, espec√≠ficamente del conjunto Financial Transactions Dataset for Fraud Detection. Se trata de un dataset real y anonimizado, ampliamente utilizado para investigaci√≥n y experimentaci√≥n en problemas de detecci√≥n de fraude financiero, el cual contiene m√°s de cinco millones de registros de transacciones.\n",
        "\n",
        "**Periodo:**\n",
        "El dataset representa transacciones financieras registradas a lo largo de un per√≠odo temporal continuo, simulado para reflejar el comportamiento real de operaciones financieras en entornos productivos. Si bien las fechas exactas no corresponden a eventos reales por motivos de anonimizaci√≥n, la variable temporal permite analizar patrones asociados al momento de la transacci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "| Variable | Tipo | Descripci√≥n |\n",
        "|---------|------|-------------|\n",
        "| transaction_id | Categ√≥rica (string) | Identificador √∫nico para cada transacci√≥n. |\n",
        "| timestamp | Temporal (string ISO) | Marca de tiempo en formato ISO cuando ocurri√≥ la transacci√≥n. |\n",
        "| sender_account | Categ√≥rica (string) | ID de cuenta del iniciador de la transacci√≥n. |\n",
        "| receiver_account | Categ√≥rica (string) | ID de cuenta del receptor de la transacci√≥n. |\n",
        "| amount | Num√©rica (float) | Monto de la transacci√≥n en USD. |\n",
        "| transaction_type | Categ√≥rica (string) | Tipo de transacci√≥n: dep√≥sito, retiro, transferencia o pago. |\n",
        "| merchant_category | Categ√≥rica (string) | Categor√≠a del negocio involucrado en la transacci√≥n (p. ej., retail, servicios p√∫blicos). |\n",
        "| location | Categ√≥rica (string) | Ubicaci√≥n desde donde se inici√≥ la transacci√≥n. |\n",
        "| device_used | Categ√≥rica (string) | Tipo de dispositivo utilizado: m√≥vil, web, cajero autom√°tico, POS. |\n",
        "| is_fraud | Booleana (boolean) | Flag booleano que indica si la transacci√≥n fue fraudulenta. |\n",
        "| fraud_type | Categ√≥rica (string) | Tipo de fraude (p. ej., lavado de dinero, suplantaci√≥n de cuenta); null si no es fraudulenta. |\n",
        "| time_since_last_transaction | Num√©rica (float) | Horas desde la transacci√≥n previa del usuario. |\n",
        "| spending_deviation_score | Num√©rica (float) | Desviaci√≥n del patr√≥n de gasto normal (distribuci√≥n gaussiana). |\n",
        "| velocity_score | Num√©rica (integer) | N√∫mero de transacciones en un per√≠odo reciente (proxy para fraude basado en velocidad). |\n",
        "| geo_anomaly_score | Num√©rica (float) | Medida de comportamiento transaccional geogr√°fico inusual (0-1). |\n",
        "| payment_channel | Categ√≥rica (string) | Canal utilizado: tarjeta, ACH, transferencia bancaria, UPI. |\n",
        "| ip_address | Categ√≥rica (string) | Direcci√≥n IPv4 simulada de la fuente de la transacci√≥n. |\n",
        "| device_hash | Categ√≥rica (string) | Hash anonimizado que representa la huella del dispositivo del usuario. |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTsA1FpvK1tw"
      },
      "source": [
        "### 4.3 Exploraci√≥n Inicial de Datos (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OYyhBsuK1tw"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# INFORMACI√ìN GENERAL DEL DATASET\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Primeras filas\n",
        "print(\"\\nüìä Primeras 5 filas:\")\n",
        "display(df.head())\n",
        "\n",
        "# Informaci√≥n del dataset\n",
        "print(\"\\nüìã Informaci√≥n del Dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "# Estad√≠sticas descriptivas\n",
        "print(\"\\nüìà Estad√≠sticas Descriptivas:\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUM7bTMZK1tx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE VALORES FALTANTES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calcular valores faltantes\n",
        "missing_data = pd.DataFrame({\n",
        "    'Total Faltantes': df.isnull().sum(),\n",
        "    'Porcentaje (%)': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "missing_data = missing_data[missing_data['Total Faltantes'] > 0].sort_values('Porcentaje (%)', ascending=False)\n",
        "\n",
        "if len(missing_data) > 0:\n",
        "    print(\"\\n‚ö†Ô∏è Variables con valores faltantes:\")\n",
        "    display(missing_data)\n",
        "\n",
        "    # Visualizaci√≥n de valores faltantes\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=missing_data.index, y='Porcentaje (%)', data=missing_data)\n",
        "    plt.title('Porcentaje de Valores Faltantes por Variable')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Porcentaje (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n‚úÖ No hay valores faltantes en el dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w54TocKK1tx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE LA VARIABLE OBJETIVO\n",
        "# =====================================================\n",
        "\n",
        "# COMPLETE: Especifique el nombre de su variable objetivo\n",
        "TARGET_COLUMN = 'target'  # Cambie 'target' por el nombre de su variable objetivo\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"AN√ÅLISIS DE LA VARIABLE OBJETIVO: {TARGET_COLUMN}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Para clasificaci√≥n\n",
        "if df[TARGET_COLUMN].dtype == 'object' or df[TARGET_COLUMN].nunique() < 20:\n",
        "    print(\"\\nüìä Distribuci√≥n de clases:\")\n",
        "    class_dist = df[TARGET_COLUMN].value_counts()\n",
        "    print(class_dist)\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Gr√°fico de barras\n",
        "    sns.countplot(data=df, x=TARGET_COLUMN, ax=axes[0])\n",
        "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
        "    axes[0].set_xlabel(TARGET_COLUMN)\n",
        "    axes[0].set_ylabel('Frecuencia')\n",
        "\n",
        "    # Gr√°fico de pastel\n",
        "    axes[1].pie(class_dist.values, labels=class_dist.index, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1].set_title(f'Proporci√≥n de {TARGET_COLUMN}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Verificar desbalance\n",
        "    imbalance_ratio = class_dist.max() / class_dist.min()\n",
        "    if imbalance_ratio > 3:\n",
        "        print(f\"\\n‚ö†Ô∏è ADVERTENCIA: Dataset desbalanceado (ratio {imbalance_ratio:.2f}:1)\")\n",
        "        print(\"   Considere t√©cnicas de balanceo: SMOTE, undersampling, class weights\")\n",
        "else:\n",
        "    # Para regresi√≥n\n",
        "    print(\"\\nüìä Estad√≠sticas de la variable objetivo:\")\n",
        "    print(df[TARGET_COLUMN].describe())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Histograma\n",
        "    sns.histplot(df[TARGET_COLUMN], kde=True, ax=axes[0])\n",
        "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
        "\n",
        "    # Box plot\n",
        "    sns.boxplot(y=df[TARGET_COLUMN], ax=axes[1])\n",
        "    axes[1].set_title(f'Box Plot de {TARGET_COLUMN}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG0MhOAJK1ty"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE CORRELACIONES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MATRIZ DE CORRELACIONES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Seleccionar solo columnas num√©ricas\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "if len(numeric_cols) > 1:\n",
        "    # Calcular correlaciones\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "                center=0, fmt='.2f', linewidths=0.5)\n",
        "    plt.title('Matriz de Correlaciones')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlaciones con la variable objetivo\n",
        "    if TARGET_COLUMN in numeric_cols:\n",
        "        print(f\"\\nüìä Correlaciones con {TARGET_COLUMN}:\")\n",
        "        target_corr = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(ascending=False)\n",
        "        print(target_corr)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay suficientes columnas num√©ricas para an√°lisis de correlaci√≥n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KttK_7FoK1tz"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# VISUALIZACIONES ADICIONALES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"VISUALIZACIONES ADICIONALES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Distribuci√≥n de variables num√©ricas\n",
        "numeric_cols_plot = df.select_dtypes(include=[np.number]).columns[:8]  # Primeras 8 columnas\n",
        "\n",
        "if len(numeric_cols_plot) > 0:\n",
        "    n_cols = 2\n",
        "    n_rows = (len(numeric_cols_plot) + 1) // 2\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
        "\n",
        "    for i, col in enumerate(numeric_cols_plot):\n",
        "        if i < len(axes):\n",
        "            sns.histplot(df[col], kde=True, ax=axes[i])\n",
        "            axes[i].set_title(f'Distribuci√≥n de {col}')\n",
        "\n",
        "    # Ocultar ejes vac√≠os\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip-qsskyK1tz"
      },
      "source": [
        "### 4.4 Hallazgos del EDA\n",
        "\n",
        "**Instrucciones:** Resuma los principales hallazgos de la exploraci√≥n de datos:\n",
        "\n",
        "---\n",
        "\n",
        "**Hallazgos Principales:**\n",
        "1. El dataset contiene **5 millones de registros y 18 variables**, combinando variables num√©ricas, categ√≥ricas y temporales, lo que lo hace adecuado para el entrenamiento de modelos de Deep Learning a gran escala.\n",
        "2. La variable objetivo `is_fraud` presenta un **fuerte desbalance de clases**, donde aproximadamente el **96.4%** de las transacciones corresponden a operaciones no fraudulentas y solo alrededor del **3.6%** son fraudulentas.\n",
        "3. Las variables num√©ricas muestran distribuciones heterog√©neas:  \n",
        "   - `amount` presenta una distribuci√≥n altamente sesgada a la derecha.  \n",
        "   - `spending_deviation_score` sigue una distribuci√≥n aproximadamente gaussiana centrada en cero.  \n",
        "   - `velocity_score` y `geo_anomaly_score` reflejan patrones discretos y normalizados, respectivamente, asociados a comportamientos an√≥malos.\n",
        "\n",
        "**Problemas Identificados:**\n",
        "1. Se identifican **valores faltantes significativos** en algunas variables, especialmente en `fraud_type` (asociado mayoritariamente a transacciones no fraudulentas) y en `time_since_last_transaction`.\n",
        "2. El **desbalance severo de la variable objetivo** puede afectar el desempe√±o del modelo si no se aplican t√©cnicas adecuadas durante el entrenamiento.\n",
        "\n",
        "**Acciones a Tomar:**\n",
        "1. Aplicar estrategias de **manejo de valores faltantes**, considerando imputaci√≥n o exclusi√≥n controlada de variables seg√∫n su relevancia para el modelado.\n",
        "2. Implementar t√©cnicas para tratar el **desbalance de clases**, como el uso de `class weights` durante el entrenamiento del modelo, priorizando m√©tricas como recall y AUC-ROC para la clase fraudulenta.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfXUT6d-K1tz"
      },
      "source": [
        "---\n",
        "## 5. Preprocesamiento de Datos\n",
        "\n",
        "### 5.1 Tratamiento de Valores Faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glq6aqxjK1t0"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# TRATAMIENTO DE VALORES FALTANTES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRATAMIENTO DE VALORES FALTANTES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Crear copia del dataframe\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Opci√≥n 1: Eliminar filas con valores faltantes\n",
        "# df_clean = df_clean.dropna()\n",
        "\n",
        "# Opci√≥n 2: Imputar con la media (variables num√©ricas)\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# imputer = SimpleImputer(strategy='mean')\n",
        "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# Opci√≥n 3: Imputar con la moda (variables categ√≥ricas)\n",
        "# for col in categorical_cols:\n",
        "#     df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
        "\n",
        "# Opci√≥n 4: Imputaci√≥n avanzada con KNN\n",
        "# from sklearn.impute import KNNImputer\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de imputaci√≥n\n",
        "# =====================================================\n",
        "\n",
        "# Eliminar columna fraud_type porque posee alta cantidad de nulos y no es una variable predictora, incluso es una variable explicativa post-fraude\n",
        "df_clean.drop(columns=['fraud_type'], inplace=True)\n",
        "\n",
        "# Imputar valor num√©rico de la variable time_since_last_transaction para entrenamiento con un \"-1\" para que de esa manera el modelo aprenda que ‚Äúno hubo transacci√≥n previa‚Äù\n",
        "df_clean['time_since_last_transaction'].fillna(-1, inplace=True)\n",
        "\n",
        "print(f\"\\n Valores faltantes tratados\")\n",
        "print(f\"   Filas restantes: {len(df_clean):,}\")\n",
        "print(\"Valores faltantes por columna:\")\n",
        "print(df_clean.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjr2sIMiK1t0"
      },
      "source": [
        "### 5.2 Tratamiento de Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa5do9ekK1t0"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DETECCI√ìN DE OUTLIERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detecta outliers usando el m√©todo IQR\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return len(outliers), lower_bound, upper_bound\n",
        "\n",
        "# Detectar outliers en cada columna num√©rica\n",
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "outlier_summary = []\n",
        "for col in numeric_cols:\n",
        "    n_outliers, lower, upper = detect_outliers_iqr(df_clean, col)\n",
        "    if n_outliers > 0:\n",
        "        outlier_summary.append({\n",
        "            'Variable': col,\n",
        "            'N_Outliers': n_outliers,\n",
        "            'Porcentaje (%)': round(n_outliers/len(df_clean)*100, 2),\n",
        "            'L√≠mite_Inferior': round(lower, 2),\n",
        "            'L√≠mite_Superior': round(upper, 2)\n",
        "        })\n",
        "\n",
        "if outlier_summary:\n",
        "    outlier_df = pd.DataFrame(outlier_summary)\n",
        "    print(\"\\n‚ö†Ô∏è Variables con outliers detectados:\")\n",
        "    display(outlier_df)\n",
        "else:\n",
        "    print(\"\\n‚úÖ No se detectaron outliers significativos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bm-abr8K1t2"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# TRATAMIENTO DE OUTLIERS (OPCIONAL)\n",
        "# =====================================================\n",
        "\n",
        "# Opci√≥n 1: Eliminar outliers\n",
        "# for col in numeric_cols:\n",
        "#     Q1, Q3 = df_clean[col].quantile([0.25, 0.75])\n",
        "#     IQR = Q3 - Q1\n",
        "#     df_clean = df_clean[(df_clean[col] >= Q1 - 1.5*IQR) & (df_clean[col] <= Q3 + 1.5*IQR)]\n",
        "\n",
        "# Opci√≥n 2: Capear outliers (winsorizing)\n",
        "# from scipy.stats import mstats\n",
        "# for col in numeric_cols:\n",
        "#     df_clean[col] = mstats.winsorize(df_clean[col], limits=[0.05, 0.05])\n",
        "\n",
        "# Opci√≥n 3: Transformaci√≥n logar√≠tmica\n",
        "# for col in cols_to_transform:\n",
        "#     df_clean[col] = np.log1p(df_clean[col])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de tratamiento\n",
        "# =====================================================\n",
        "\n",
        "# En esta secci√≥n del proyecto no se eliminar√°n ni recortar√°n outliers.\n",
        "# Los valores extremos representan comportamientos an√≥malos que pueden estar asociados a transacciones fraudulentas.\n",
        "\n",
        "print(\"No se aplic√≥ eliminaci√≥n ni recorte de outliers.\")\n",
        "print(\"Los outliers ser√°n manejados posteriormente mediante t√©cnicas de escalado robusto.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkljPM0GK1t2"
      },
      "source": [
        "### 5.3 Codificaci√≥n de Variables Categ√≥ricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap4LulvtK1t3"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Identificar variables categ√≥ricas\n",
        "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(f\"\\nVariables categ√≥ricas encontradas: {categorical_cols}\")\n",
        "\n",
        "# Opci√≥n 1: Label Encoding (para variables ordinales o target)\n",
        "# le = LabelEncoder()\n",
        "# df_clean['columna_encoded'] = le.fit_transform(df_clean['columna'])\n",
        "\n",
        "# Opci√≥n 2: One-Hot Encoding (para variables nominales)\n",
        "# df_clean = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Opci√≥n 3: Target Encoding\n",
        "# from sklearn.preprocessing import TargetEncoder\n",
        "# encoder = TargetEncoder()\n",
        "# df_clean[categorical_cols] = encoder.fit_transform(df_clean[categorical_cols], df_clean[TARGET_COLUMN])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de codificaci√≥n\n",
        "# =====================================================\n",
        "# Eliminar identificadores de alta cardinalidad ya que son identificadores t√©cnicos sin significado predictivo directo\n",
        "id_cols = [\n",
        "    'transaction_id',\n",
        "    'sender_account',\n",
        "    'receiver_account',\n",
        "    'ip_address',\n",
        "    'device_hash'\n",
        "]\n",
        "df_clean.drop(columns=id_cols, inplace=True)\n",
        "\n",
        "# Reducir cardinalidad en variables grandes\n",
        "def reduce_cardinality(series, min_freq=0.01):\n",
        "    freq = series.value_counts(normalize=True)\n",
        "    common = freq[freq >= min_freq].index\n",
        "    return series.where(series.isin(common), 'OTHER')\n",
        "\n",
        "high_card_cols = ['location', 'merchant_category']\n",
        "for col in high_card_cols:\n",
        "    df_clean[col] = reduce_cardinality(df_clean[col], min_freq=0.01)\n",
        "\n",
        "# Variables categ√≥ricas a codificar\n",
        "categorical_cols = [\n",
        "    'transaction_type',\n",
        "    'merchant_category',\n",
        "    'location',\n",
        "    'device_used',\n",
        "    'payment_channel'\n",
        "]\n",
        "\n",
        "print(f\"Variables categ√≥ricas a codificar: {categorical_cols}\")\n",
        "\n",
        "# One-Hot Encoding optimizado\n",
        "df_clean = pd.get_dummies(\n",
        "    df_clean,\n",
        "    columns=categorical_cols,\n",
        "    drop_first=True,\n",
        "    dtype='int8'\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Codificaci√≥n completada\")\n",
        "print(f\"   Dimensiones finales: {df_clean.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cn8NERlK1t3"
      },
      "source": [
        "### 5.4 Escalado/Normalizaci√≥n de Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dreVJK9VK1t3"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ESCALADO DE FEATURES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ESCALADO DE FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Separar features y target\n",
        "X = df_clean.drop(columns=[TARGET_COLUMN, 'timestamp'])\n",
        "y = df_clean[TARGET_COLUMN]\n",
        "\n",
        "print(f\"\\nDimensiones de X: {X.shape}\")\n",
        "print(f\"Dimensiones de y: {y.shape}\")\n",
        "\n",
        "# Opci√≥n 1: StandardScaler (media=0, std=1) - Recomendado para redes neuronales\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# Opci√≥n 2: MinMaxScaler (rango [0,1])\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# Opci√≥n 3: RobustScaler (robusto a outliers)\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# scaler = RobustScaler()\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Aplicar RobustScaler (robusto a outliers)\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Aplicar escalado\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(f\"\\n‚úÖ Escalado completado usando {type(scaler).__name__}\")\n",
        "print(f\"   Media de features: {X_scaled.mean().mean():.6f}\")\n",
        "print(f\"   Std de features: {X_scaled.std().mean():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex7ppKX_K1t4"
      },
      "source": [
        "### 5.5 Divisi√≥n de Datos (Train/Validation/Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmqHXKydK1t4"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DIVISI√ìN DE DATOS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DIVISI√ìN DE DATOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Divisi√≥n en train (70%), validation (15%), test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.15, random_state=RANDOM_SEED, stratify=y if y.dtype == 'object' or y.nunique() < 20 else None\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.176, random_state=RANDOM_SEED, stratify=y_temp if y_temp.dtype == 'object' or y_temp.nunique() < 20 else None  # 0.176 ‚âà 15% del total\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
        "print(f\"   Training set:   {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"   Validation set: {X_val.shape[0]:,} muestras ({X_val.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"   Test set:       {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "\n",
        "# Verificar distribuci√≥n de clases (para clasificaci√≥n)\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    print(f\"\\nüìä Distribuci√≥n de clases en cada conjunto:\")\n",
        "    print(f\"   Train: {dict(y_train.value_counts(normalize=True).round(3))}\")\n",
        "    print(f\"   Val:   {dict(y_val.value_counts(normalize=True).round(3))}\")\n",
        "    print(f\"   Test:  {dict(y_test.value_counts(normalize=True).round(3))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRygEdo-K1t4"
      },
      "source": [
        "### 5.6 Preparaci√≥n de Datos para Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlHUXBOdK1t5"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# PREPARACI√ìN PARA TENSORFLOW/KERAS (ALTERNATIVA)\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPARACI√ìN DE DATOS PARA TENSORFLOW/KERAS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convertir a arrays numpy (Keras acepta DataFrames directamente, pero es mejor convertir)\n",
        "X_train_np = X_train.values.astype('float32')\n",
        "X_val_np = X_val.values.astype('float32')\n",
        "X_test_np = X_test.values.astype('float32')\n",
        "\n",
        "# Para clasificaci√≥n: One-hot encoding del target\n",
        "\n",
        "y_train_np = y_train.values.astype('float32')\n",
        "y_val_np = y_val.values.astype('float32')\n",
        "y_test_np = y_test.values.astype('float32')\n",
        "\n",
        "print(f\"\\n‚úÖ Datos preparados para TensorFlow/Keras\")\n",
        "print(f\"   Shape X_train: {X_train_np.shape}\")\n",
        "print(f\"   Shape y_train: {y_train_np.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DsWBdjjK1t5"
      },
      "source": [
        "---\n",
        "## 6. Dise√±o y Arquitectura del Modelo\n",
        "\n",
        "### 6.1 Justificaci√≥n de la Arquitectura\n",
        "\n",
        "**Instrucciones:** Justifique la elecci√≥n de su arquitectura de red neuronal:\n",
        "- ¬øPor qu√© eligi√≥ este tipo de arquitectura?\n",
        "- ¬øQu√© alternativas consider√≥?\n",
        "- ¬øC√≥mo determin√≥ el n√∫mero de capas y neuronas?\n",
        "\n",
        "---\n",
        "\n",
        "Se eligi√≥ una arquitectura de Red Neuronal Multicapa (Multilayer Perceptron - MLP) debido a que el problema abordado corresponde a detecci√≥n de fraude en datos estructurados/tabulares, donde las variables de entrada incluyen caracter√≠sticas num√©ricas escaladas y variables categ√≥ricas codificadas mediante one-hot encoding. Este tipo de arquitectura es ampliamente utilizada y efectiva para modelar relaciones no lineales complejas entre variables en datasets tabulares de gran tama√±o, como el utilizado en este proyecto.\n",
        "\n",
        "La elecci√≥n del MLP se alinea con la naturaleza del dataset, el cual contiene cinco millones de transacciones financieras y presenta un alto desbalance de clases, patrones no lineales y se√±ales de fraude asociadas a combinaciones espec√≠ficas de variables (por ejemplo, monto, velocidad transaccional y desviaci√≥n de gasto). Un modelo MLP permite capturar estas interacciones sin necesidad de realizar ingenier√≠a manual de combinaciones de variables.\n",
        "\n",
        "Como alternativas, se consideraron otros enfoques como modelos cl√°sicos de machine learning (Logistic Regression, Random Forest, Gradient Boosting) y arquitecturas m√°s complejas como LSTM o modelos basados en secuencias. Sin embargo, estos √∫ltimos fueron descartados debido a que el dataset no se encuentra estructurado expl√≠citamente como una serie temporal secuencial por usuario, y el objetivo del proyecto es demostrar un pipeline de Deep Learning escalable en la nube sobre datos tabulares. Asimismo, modelos basados en √°rboles, si bien efectivos, no cumplen con el requerimiento principal del proyecto de entrenar un modelo de Deep Learning utilizando GPU.\n",
        "\n",
        "El n√∫mero de capas y neuronas se determin√≥ siguiendo un enfoque progresivo y emp√≠rico, partiendo de una arquitectura sencilla pero suficientemente expresiva. Se opt√≥ por m√∫ltiples capas densas con una reducci√≥n gradual del n√∫mero de neuronas, lo que permite aprender representaciones jer√°rquicas de los datos y reducir el riesgo de sobreajuste. Este dise√±o se complementa posteriormente con t√©cnicas de regularizaci√≥n y validaci√≥n, ajustando la complejidad del modelo en funci√≥n del desempe√±o observado en el conjunto de validaci√≥n.\n",
        "\n",
        "En conjunto, la arquitectura seleccionada representa un balance adecuado entre capacidad de aprendizaje, eficiencia computacional y alineaci√≥n con los objetivos del negocio, permitiendo detectar transacciones fraudulentas de manera efectiva en un entorno de datos a gran escala.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2 Definici√≥n del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztGDQ2isK1t6"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DEFINICI√ìN DEL MODELO CON KERAS (ALTERNATIVA)\n",
        "# =====================================================\n",
        "\n",
        "def create_keras_model(input_shape, hidden_sizes, output_size, dropout_rate=0.3, task='classification'):\n",
        "    \"\"\"\n",
        "    Crea un modelo de red neuronal con Keras.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Dimensi√≥n de entrada\n",
        "        hidden_sizes: Lista con el n√∫mero de neuronas por capa oculta\n",
        "        output_size: N√∫mero de neuronas de salida\n",
        "        dropout_rate: Tasa de dropout\n",
        "        task: 'classification' o 'regression'\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Capa de entrada\n",
        "    model.add(layers.Input(shape=(input_shape,)))\n",
        "\n",
        "    # Capas ocultas\n",
        "    for hidden_size in hidden_sizes:\n",
        "        model.add(layers.Dense(hidden_size))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Capa de salida\n",
        "    if task == 'classification':\n",
        "        if output_size == 2:\n",
        "            model.add(layers.Dense(1, activation='sigmoid'))\n",
        "        else:\n",
        "            model.add(layers.Dense(output_size, activation='softmax'))\n",
        "    else:\n",
        "        model.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Crear modelo Keras\n",
        "TASK = 'classification'  # Cambie a 'regression' si es necesario\n",
        "INPUT_SIZE = X_train_np.shape[1]   # n√∫mero de features\n",
        "HIDDEN_SIZES = [128, 64, 32]        # capas ocultas\n",
        "OUTPUT_SIZE = 2                    # binario (fraude / no fraude)\n",
        "DROPOUT_RATE = 0.3\n",
        "\n",
        "model_keras = create_keras_model(\n",
        "    input_shape=INPUT_SIZE,\n",
        "    hidden_sizes=HIDDEN_SIZES,\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    task=TASK\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ARQUITECTURA DEL MODELO (Keras)\")\n",
        "print(\"=\" * 60)\n",
        "model_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ajyHcHCK1uC"
      },
      "source": [
        "### 6.3 Diagrama de la Arquitectura\n",
        "\n",
        "**Instrucciones:** Incluya un diagrama visual de su arquitectura de red neuronal.\n",
        "\n",
        "---\n",
        "\n",
        "Arquitectura de la Red Neuronal Multicapa (MLP) para Detecci√≥n de Fraude\n",
        "\n",
        "Input Layer  \n",
        "- Dimensi√≥n: n_features  \n",
        "- Incluye variables num√©ricas escaladas y variables categ√≥ricas codificadas mediante one-hot encoding  \n",
        "\n",
        "    ‚Üì\n",
        "\n",
        "Hidden Layer 1  \n",
        "- Capa densa con 128 neuronas  \n",
        "- Batch Normalization  \n",
        "- Funci√≥n de activaci√≥n: ReLU  \n",
        "- Dropout (0.3)  \n",
        "\n",
        "    ‚Üì\n",
        "\n",
        "Hidden Layer 2  \n",
        "- Capa densa con 64 neuronas  \n",
        "- Batch Normalization  \n",
        "- Funci√≥n de activaci√≥n: ReLU  \n",
        "- Dropout (0.3)  \n",
        "\n",
        "    ‚Üì\n",
        "\n",
        "Hidden Layer 3  \n",
        "- Capa densa con 32 neuronas  \n",
        "- Funci√≥n de activaci√≥n: ReLU  \n",
        "\n",
        "    ‚Üì\n",
        "\n",
        "Output Layer  \n",
        "- Capa densa con 1 neurona  \n",
        "- Funci√≥n de activaci√≥n: Sigmoid  \n",
        "- Tipo de problema: Clasificaci√≥n binaria (fraude / no fraude)\n",
        "\n",
        "\n",
        "```\n",
        "Input Layer          Hidden Layer 1       Hidden Layer 2       Output Layer\n",
        "[n features]   -->   [128 neurons]   -->  [64 neurons]    -->  [n classes]\n",
        "                     + BatchNorm          + BatchNorm\n",
        "                     + ReLU               + ReLU\n",
        "                     + Dropout(0.3)       + Dropout(0.3)\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1XXzbcaK1uD"
      },
      "source": [
        "---\n",
        "## 7. Entrenamiento del Modelo\n",
        "\n",
        "### 7.1 Configuraci√≥n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHmr40PCK1uE"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# HIPERPAR√ÅMETROS DE ENTRENAMIENTO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURACI√ìN DEL ENTRENAMIENTO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "\n",
        "print(f\"\\nüìã Hiperpar√°metros:\")\n",
        "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B342YPbHK1uG"
      },
      "source": [
        "### 7.3 Entrenamiento del Modelo (Keras - Alternativa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GnTF7ZbK1uG"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ENTRENAMIENTO DEL MODELO (KERAS)\n",
        "# =====================================================\n",
        "\n",
        "# Compilar modelo\n",
        "if TASK == 'classification':\n",
        "    if OUTPUT_SIZE == 2:\n",
        "        model_keras.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "    else:\n",
        "        model_keras.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "else:\n",
        "    model_keras.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "# Callbacks\n",
        "keras_callbacks = [\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ModelCheckpoint(\n",
        "        'best_model.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "# Entrenar\n",
        "print(\"=\" * 60)\n",
        "print(\"ENTRENAMIENTO DEL MODELO (KERAS)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes,\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "class_weight = dict(zip(classes, class_weights))\n",
        "print(\"Class weights:\", class_weight)\n",
        "\n",
        "history = model_keras.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    validation_data=(X_val_np, y_val_np),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=keras_callbacks,\n",
        "    class_weight=class_weight,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nüéâ Entrenamiento completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYzRckURK1uH"
      },
      "source": [
        "### 7.4 Visualizaci√≥n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD0lFnA3K1uH"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# VISUALIZACI√ìN DEL PROCESO DE ENTRENAMIENTO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CURVAS DE APRENDIZAJE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Gr√°fico de p√©rdida\n",
        "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_title('Evoluci√≥n de la P√©rdida', fontsize=14)\n",
        "axes[0].set_xlabel('√âpoca')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico de precisi√≥n (solo para clasificaci√≥n)\n",
        "#if task_type == 'classification':\n",
        "axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[1].set_title('Evoluci√≥n de la Precisi√≥n', fontsize=14)\n",
        "axes[1].set_xlabel('√âpoca')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "#else:\n",
        "#axes[1].text(0.5, 0.5, 'N/A para Regresi√≥n', ha='center', va='center', fontsize=14)\n",
        "#axes[1].set_title('Precisi√≥n (No aplica)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lisis del entrenamiento\n",
        "print(\"\\ An√°lisis del Entrenamiento:\")\n",
        "print(f\"   √âpocas completadas: {len(history.history['loss'])}\")\n",
        "print(f\"   Mejor val_loss: {min(history.history['val_loss']):.4f} (√©poca {history.history['val_loss'].index(min(history.history['val_loss']))+1})\")\n",
        "#if task_type == 'classification':\n",
        "print(f\"   Mejor val_acc: {max(history.history['val_accuracy']):.4f} (√©poca {history.history['val_accuracy'].index(max(history.history['val_accuracy']))+1})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re7h9F2qK1uH"
      },
      "source": [
        "---\n",
        "## 8. Evaluaci√≥n y M√©tricas\n",
        "\n",
        "### 8.1 Evaluaci√≥n en el Conjunto de Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-smHDUmeK1uI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Probabilidades del modelo DL\n",
        "y_proba = model_keras.predict(X_test_np, batch_size=2048).ravel()\n",
        "\n",
        "# Umbral\n",
        "threshold = 0.5\n",
        "y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"PR-AUC:\", average_precision_score(y_test, y_proba))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1:\", f1_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6634up5NK1uI"
      },
      "source": [
        "### 8.2 Comparaci√≥n con Modelo Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj4SuodVK1uJ"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# COMPARACI√ìN CON MODELO BASELINE\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARACI√ìN CON MODELO BASELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if TASK == 'classification':\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1) MUESTREO PARA BASELINES PARA EVITAR CRASHEOS DEL GPU\n",
        "    # -----------------------------\n",
        "    SAMPLE_N = 300000  # prueba 200k‚Äì500k seg√∫n RAM/tiempo\n",
        "    rng = np.random.RandomState(RANDOM_SEED)\n",
        "    idx = rng.choice(X_train.index, size=min(SAMPLE_N, len(X_train)), replace=False)\n",
        "\n",
        "    X_train_s = X_train.loc[idx]\n",
        "    y_train_s = y_train.loc[idx]\n",
        "\n",
        "    # Modelos baseline (m√°s livianos)\n",
        "    baselines = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=300, n_jobs=-1),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=50, random_state=RANDOM_SEED, n_jobs=-1)\n",
        "    }\n",
        "\n",
        "    metric_name = \"ROC-AUC\"\n",
        "\n",
        "else:\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.metrics import r2_score\n",
        "\n",
        "    baselines = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=50, random_state=RANDOM_SEED, n_jobs=-1)\n",
        "    }\n",
        "\n",
        "    metric_name = \"R¬≤\"\n",
        "\n",
        "\n",
        "# Entrenar y evaluar baselines\n",
        "results = {'Modelo': [], 'M√©trica': []}\n",
        "\n",
        "for name, model in baselines.items():\n",
        "\n",
        "    if TASK == 'classification':\n",
        "        print(f\"\\nEntrenando baseline: {name} (muestra={len(X_train_s):,})\")\n",
        "        model.fit(X_train_s, y_train_s)\n",
        "\n",
        "        # Probabilidades -> AUC\n",
        "        y_proba_baseline = model.predict_proba(X_test)[:, 1]\n",
        "        metric = roc_auc_score(y_test, y_proba_baseline)\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nEntrenando baseline: {name}\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_baseline = model.predict(X_test)\n",
        "        metric = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "    results['Modelo'].append(name)\n",
        "    results['M√©trica'].append(metric)\n",
        "\n",
        "\n",
        "# Agregar modelo de Deep Learning\n",
        "results['Modelo'].append('Deep Learning (MLP)')\n",
        "\n",
        "if TASK == 'classification':\n",
        "    # Probabilidades del modelo DL -> AUC\n",
        "    y_proba_dl = model_keras.predict(X_test_np, batch_size=2048).ravel()\n",
        "    dl_metric = roc_auc_score(y_test, y_proba_dl)\n",
        "else:\n",
        "    dl_metric = r2\n",
        "\n",
        "results['M√©trica'].append(dl_metric)\n",
        "\n",
        "\n",
        "# Mostrar comparaci√≥n\n",
        "comparison_df = pd.DataFrame(results).sort_values('M√©trica', ascending=False)\n",
        "\n",
        "print(f\"\\nüìä Comparaci√≥n de Modelos ({metric_name}):\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(comparison_df['Modelo'], comparison_df['M√©trica'])\n",
        "plt.xlabel(metric_name)\n",
        "plt.title(f'Comparaci√≥n de Modelos - {metric_name}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCANdrFfK1uK"
      },
      "source": [
        "### 8.3 An√°lisis de Resultados\n",
        "\n",
        "**Instrucciones:** Analice los resultados obtenidos:\n",
        "\n",
        "---\n",
        "\n",
        "**Rendimiento del Modelo:**\n",
        "\n",
        "En el conjunto de test, el modelo MLP obtuvo un ROC-AUC ‚âà 0.590 y PR-AUC ‚âà 0.044, lo que indica una capacidad de discriminaci√≥n ligeramente superior al azar, pero todav√≠a limitada para un caso cr√≠tico como fraude. Con un umbral de 0.5, la precisi√≥n para la clase fraude es baja (‚âà 0.044), mientras que el recall de fraude es alto (‚âà 0.91). Esto significa que el modelo detecta la mayor√≠a de fraudes, pero a costa de generar muchos falsos positivos (muchas transacciones leg√≠timas marcadas como fraude). La matriz de confusi√≥n muestra este trade-off: TP=24,434 y FN=2,449 (buen recall), pero FP=536,028 (muy alto), lo cual impacta directamente en operaci√≥n (alertas innecesarias y fricci√≥n al cliente).\n",
        "\n",
        "**Comparaci√≥n con Baselines:**\n",
        "\n",
        "En la comparaci√≥n por ROC-AUC, el modelo Deep Learning (MLP) fue el mejor: 0.589970, superando a Random Forest (0.566383) y a Logistic Regression (0.498507). Esto sugiere que el MLP logra capturar relaciones no lineales en las variables disponibles, aportando valor frente a modelos tradicionales. Sin embargo, aunque supera a los baselines, el nivel de desempe√±o todav√≠a deja espacio importante de mejora para un sistema de detecci√≥n de fraude en producci√≥n.\n",
        "\n",
        "**Fortalezas del Modelo:**\n",
        "\n",
        "1. Alto recall en fraude (‚âà 0.91): √∫til para priorizar la detecci√≥n de eventos fraudulentos minimizando fraudes no detectados.\n",
        "2. Mejor desempe√±o global (ROC-AUC) vs baselines: el MLP supera a Random Forest y Logistic Regression, indicando mejor capacidad de aprendizaje en el espacio de features.\n",
        "\n",
        "**Debilidades del Modelo:**\n",
        "\n",
        "1. Muy baja precisi√≥n en fraude (‚âà 0.044) y alto FP: genera un volumen elevado de alertas falsas, lo que puede saturar equipos de revisi√≥n y afectar la experiencia del cliente.\n",
        "2. PR-AUC bajo (‚âà 0.044): evidencia dificultad para separar bien la clase minoritaria en un contexto desbalanceado, donde esta m√©trica es m√°s representativa que accuracy.\n",
        "\n",
        "**Posibles Mejoras:**\n",
        "\n",
        "1. Optimizar el umbral y usar m√©tricas orientadas a fraude: ajustar el threshold (no necesariamente 0.5) buscando un balance negocio (ej. maximizar F1 o lograr un recall objetivo con FP controlado), adem√°s de usar PR curve para seleccionar el punto operativo.\n",
        "2. Manejo del desbalance y enriquecimiento de features: aplicar class_weight, focal loss o t√©cnicas de re-muestreo (undersampling/SMOTE con cuidado), e incorporar variables derivadas (features temporales del timestamp, agregados por cuenta/dispositivo, ventanas de transacciones, frecuencia por canal/ubicaci√≥n) para dar m√°s se√±al al modelo y mejorar la precisi√≥n.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf7O74CEK1uK"
      },
      "source": [
        "---\n",
        "## 9. Interpretaci√≥n de Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrg44Vy5K1uK"
      },
      "source": [
        "### 9.2 Interpretaci√≥n de Negocios\n",
        "\n",
        "**Instrucciones:** Traduzca los resultados t√©cnicos a insights de negocio:\n",
        "\n",
        "---\n",
        "\n",
        "**Insights Principales:**\n",
        "1. El modelo es excelente \"atrapando\" al defraudador, lo cual es positivo para reducir p√©rdidas directas. Sin embargo, el costo es una fricci√≥n masiva: por cada fraude real detectado, estamos bloqueando o revisando incorrectamente a 22 clientes leg√≠timos. Esto puede causar abandono de la plataforma y saturaci√≥n del equipo de soporte.\n",
        "2. El hecho de que el modelo MLP (Deep Learning) supere a Random Forest y Regresi√≥n Log√≠stica indica que el fraude en este dataset no sigue reglas simples o lineales. El negocio debe seguir invirtiendo en modelos avanzados, ya que las reglas manuales tradicionales probablemente fallar√≠an en detectar los patrones complejos que el MLP s√≠ ve.\n",
        "3. Operar el modelo con un umbral de 0.5 no es viable para el negocio debido al volumen de falsos positivos (m√°s de 536,000 alertas innecesarias). Se requiere una segmentaci√≥n operativa: tratar de forma distinta las alertas de \"alto riesgo\" de las de \"riesgo moderado\" para no colapsar la operaci√≥n.\n",
        "\n",
        "**Factores M√°s Importantes:**\n",
        "Dado que el MLP fue el mejor modelo, los factores m√°s importantes no son variables aisladas, sino la interacci√≥n entre ellas. Denota un comportamiento no lineal.\n",
        "\n",
        "**Patrones Identificados:**\n",
        "El patr√≥n m√°s claro es el enorme solapamiento entre la clase leg√≠tima y la fraudulenta. El modelo identifica que el fraude intenta mimetizarse perfectamente con las transacciones comunes, lo que explica el bajo PR-AUC.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKrgAlyNK1uL"
      },
      "source": [
        "---\n",
        "## 10. Conclusiones y Recomendaciones de Negocio\n",
        "\n",
        "### 10.1 Resumen de Resultados\n",
        "\n",
        "**Instrucciones:** Proporcione un resumen ejecutivo de los resultados:\n",
        "\n",
        "---\n",
        "\n",
        "El proyecto de detecci√≥n de fraude se centr√≥ en la implementaci√≥n de un modelo de Deep Learning (MLP), el cual demostr√≥ ser superior a los modelos tradicionales (Random Forest y Regresi√≥n Log√≠stica) al alcanzar un ROC-AUC de 0.59. Este resultado confirma que el fraude en este conjunto de datos presenta patrones complejos y no lineales que las redes neuronales logran capturar con mayor eficacia que los m√©todos lineales o basados en √°rboles.\n",
        "\n",
        "Sin embargo, el modelo actual opera bajo un esquema de \"red amplia\": logra capturar el 91% de los eventos fraudulentos (Recall), pero a costa de una precisi√≥n extremadamente baja (0.044). En t√©rminos operativos, esto se traduce en una capacidad de detecci√≥n muy alta, pero con un volumen de falsos positivos que supera las 536,000 transacciones leg√≠timas marcadas err√≥neamente, lo que representa el principal desaf√≠o para su implementaci√≥n en un entorno de producci√≥n real.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.2 Conclusiones\n",
        "\n",
        "**Instrucciones:** Liste las conclusiones principales:\n",
        "\n",
        "---\n",
        "\n",
        "1. La arquitectura MLP super√≥ a los baselines, demostrando que existe valor en utilizar modelos de mayor capacidad para identificar se√±ales sutiles de fraude ocultas en los datos.\n",
        "\n",
        "2. El modelo es altamente sensible al fraude. Si bien es positivo no dejar pasar casi ning√∫n evento criminal, la configuraci√≥n actual es \"demasiado nerviosa\", priorizando la seguridad sobre la experiencia del usuario leg√≠timo.\n",
        "\n",
        "3. El umbral est√°ndar de 0.5 no es el √≥ptimo para este negocio. La relaci√≥n actual de falsos positivos (1 fraude real por cada 22 alertas falsas) generar√≠a una fricci√≥n insostenible en el servicio al cliente y en los equipos de auditor√≠a.\n",
        "\n",
        "4. El bajo PR-AUC (0.044) sugiere que, aunque el modelo es mejor que el azar, las variables actuales no son suficientes para separar con nitidez el fraude del comportamiento leg√≠timo inusual.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.3 Recomendaciones de Negocio\n",
        "\n",
        "**Instrucciones:** Proporcione recomendaciones accionables basadas en los resultados:\n",
        "\n",
        "---\n",
        "\n",
        "**Recomendaciones a Corto Plazo:**\n",
        "1. No utilizar el modelo para \"bloqueos autom√°ticos\" inmediatos. Se recomienda elevar el umbral de decisi√≥n o implementar una estrategia de \"Step-up Authentication\" (pedir un segundo factor de autenticaci√≥n) solo a las alertas de mayor probabilidad en lugar de rechazar la transacci√≥n.\n",
        "\n",
        "2. Dividir las salidas del modelo en tres niveles de riesgo (Bajo, Medio, Alto) para que el equipo de operaciones se enfoque solo en el percentil superior, reduciendo la carga de trabajo manual.\n",
        "\n",
        "**Recomendaciones a Mediano Plazo:**\n",
        "1. Enriquecer el dataset con variables de \"velocidad\" (n√∫mero de transacciones en la √∫ltima hora) y \"perfilamiento\" (desviaci√≥n del monto promedio del usuario) para ayudar al modelo a distinguir mejor entre un gasto leg√≠timo inusual y un ataque.\n",
        "\n",
        "2. Re-entrenar el modelo penalizando m√°s fuertemente los Falsos Positivos o utilizando funciones de p√©rdida dise√±adas para datos desbalanceados como Focal Loss.\n",
        "\n",
        "**Recomendaciones a Largo Plazo:**\n",
        "1. Establecer un proceso donde los resultados de las revisiones manuales alimenten el modelo semanalmente para que este aprenda de sus errores y se adapte a las nuevas t√°cticas de los defraudadores.\n",
        "\n",
        "2. Considerar un sistema de votaci√≥n (Ensemble) que combine la sensibilidad del MLP con la precisi√≥n de un Random Forest bien optimizado para equilibrar la balanza entre detecci√≥n y fricci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.4 Limitaciones del Estudio\n",
        "\n",
        "**Instrucciones:** Identifique las limitaciones de su an√°lisis:\n",
        "\n",
        "---\n",
        "\n",
        "1. La escasez de ejemplos de fraude reales dificulta que el modelo aprenda con precisi√≥n los l√≠mites de la clase minoritaria sin sobre-generalizar.\n",
        "\n",
        "2. El an√°lisis actual trata las transacciones de forma aislada, perdiendo la riqueza de la secuencia de eventos que suele ser clave en los patrones de fraude modernos.\n",
        "\n",
        "3. La ausencia de datos de geolocalizaci√≥n, huella digital del dispositivo o comportamiento biom√©trico limita la capacidad del modelo para detectar usurpaciones de identidad.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.5 Trabajo Futuro\n",
        "\n",
        "**Instrucciones:** Proponga l√≠neas de investigaci√≥n futura:\n",
        "\n",
        "---\n",
        "\n",
        "1. Explorar el uso de Redes Neuronales Recurrentes para analizar el historial del cliente como una serie temporal, detectando cambios bruscos de comportamiento.\n",
        "\n",
        "2. Implementar m√©todos avanzados de generaci√≥n de datos sint√©ticos (como SMOTE o GANs) para equilibrar el entrenamiento y mejorar la precisi√≥n en la clase de fraude.\n",
        "\n",
        "3. Utilizar herramientas como SHAP o LIME para entender qu√© variables est√°n disparando las alertas de fraude, permitiendo que los analistas humanos tomen decisiones m√°s r√°pidas y fundamentadas.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSq3E86AK1uL"
      },
      "source": [
        "---\n",
        "## 11. Referencias\n",
        "\n",
        "**Instrucciones:** Liste todas las referencias utilizadas (formato APA):\n",
        "\n",
        "---\n",
        "\n",
        "1. Goodfellow, I., Bengio, Y., y Courville, A. (2016). Deep Learning. MIT Press.\n",
        "\n",
        "\n",
        "2. Fern√°ndez, A., Garc√≠a, S., Galar, M., Prati, R. C., Krawczyk, B., y Herrera, F. (2018). Learning from Imbalanced Data Sets. Springer International Publishing.\n",
        "\n",
        "\n",
        "3. Saito, T., y Rehmsmeier, M. (2015). The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets. PLOS ONE, 10(3), e0118432.\n",
        "\n",
        "\n",
        "4. Hilal, W., Gadsden, S. A., y Yawney, J. (2022). Financial Fraud: A Review of Anomaly Detection Techniques and Recent Advances. Expert Systems with Applications, 116429.\n",
        "\n",
        "\n",
        "5. Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861-874.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig1GjmfeK1uM"
      },
      "source": [
        "---\n",
        "\n",
        "## Checklist de Entrega\n",
        "\n",
        "Antes de entregar, verifique que ha completado los siguientes elementos:\n",
        "\n",
        "- [X] Informaci√≥n del proyecto completada\n",
        "- [X] Resumen ejecutivo escrito\n",
        "- [X] Problema de negocio claramente definido\n",
        "- [X] Objetivos SMART establecidos\n",
        "- [X] EDA completo con visualizaciones\n",
        "- [X] Preprocesamiento de datos documentado\n",
        "- [X] Arquitectura del modelo justificada\n",
        "- [X] Modelo entrenado con curvas de aprendizaje\n",
        "- [X] M√©tricas de evaluaci√≥n calculadas\n",
        "- [X] Comparaci√≥n con modelos baseline\n",
        "- [X] Interpretaci√≥n de resultados\n",
        "- [X] Conclusiones y recomendaciones de negocio\n",
        "- [X] Referencias listadas\n",
        "- [X] C√≥digo ejecutable sin errores\n",
        "- [X] Comentarios y documentaci√≥n adecuados\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Buena suerte con su proyecto!** üéì"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!jupyter nbconvert --to html /content/final_project.ipynb"
      ],
      "metadata": {
        "id": "D2KGkPZMuoJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"final_project.html\")\n"
      ],
      "metadata": {
        "id": "4HQZUMTHutHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}